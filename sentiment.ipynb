{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;78mclean_tweet.csv\u001b[0m  \u001b[38;5;105mtrainingandtestdata\u001b[0m/     \u001b[38;5;167mUntitled.ipynb\u001b[0m\r\n",
      "\u001b[38;5;167msentiment.ipynb\u001b[0m  \u001b[38;5;208mtrainingandtestdata.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;78mclean_tweet.csv\u001b[0m  \u001b[38;5;105mtrainingandtestdata\u001b[0m/     \u001b[38;5;167mUntitled.ipynb\u001b[0m\r\n",
      "\u001b[38;5;167msentiment.ipynb\u001b[0m  \u001b[38;5;208mtrainingandtestdata.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data (this training data is sentiment140, exctracted by stanford university)\n",
    "this is a data with sentiments with values 0, 4, for negetive and positive sentiments , respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a3ae69651052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'query_string'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Installs/anaconda3/envs/forDL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Installs/anaconda3/envs/forDL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Installs/anaconda3/envs/forDL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Installs/anaconda3/envs/forDL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",header=None, names=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b425af033ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for lengths of tweet more than 140 character, which is unsusall, as twitter allows only 140 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awwh babs... you look so sad underneith that shop entrance of &quot;Yesterday's Musik&quot;  O-: I like the look of the new transformer movie  __ 142\n",
      "Tuesday�ll start with reflection �n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers  __ 141\n",
      "Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly.  __ 145\n",
      "@TheLeagueSF Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down  __ 145\n",
      "#3 woke up and was having an accident - &quot;It's pushing, it's pushing!&quot; he was crying because he couldn't stop from wetting his pants.   __ 144\n",
      "My bathtub drain is fired: it haz 1 job 2 do, &amp; it iz FAIL. I got all Drano on its ass, &amp; iz STILL NOT DRAINING. I wanna shower, dangit!!  __ 146\n",
      "pears &amp; Brie, bottle of Cabernet, and &quot;Win a Date With Tad Hamilton&quot;... oh gawwd my life flashed forward to when I'm 40 with my 75 cats  __ 150\n",
      "Have an invite for &quot;Healthy Dining&quot; session at Ashok Hotel today with Exec Chef R.Chopra but damn workload - will have to skip it!  __ 141\n",
      "Damnit I was really digging this season of Reaper  http://www.tv.com/story/13720.html?ref_story_id=13720&amp;ref_type=1101&amp;ref_name=story __ 141\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    if df.pre_clean_len[i] > 140:\n",
    "        print df.text[i] + \" __ \" + str(df.pre_clean_len[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, html decoding is neccessary to clean the tweets, and also some urls and user id would have to be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data dictionary to easy understanding of our data, and from concise explanation to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_shape': (1600000, 3),\n",
      " 'pre_clean_len': {'description': 'Length of the tweet before cleaning',\n",
      "                   'type': dtype('int64')},\n",
      " 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',\n",
      "               'type': dtype('int64')},\n",
      " 'text': {'description': 'tweet text', 'type': dtype('O')}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "data_dict = {\n",
    "    'sentiment':{\n",
    "        'type':df.sentiment.dtype,\n",
    "        'description':'sentiment class - 0:negative, 1:positive'\n",
    "    },\n",
    "    'text':{\n",
    "        'type':df.text.dtype,\n",
    "        'description':'tweet text'\n",
    "    },\n",
    "    'pre_clean_len':{\n",
    "        'type':df.pre_clean_len.dtype,\n",
    "        'description':'Length of the tweet before cleaning'\n",
    "    },\n",
    "    'dataset_shape':df.shape\n",
    "}\n",
    "\n",
    "pprint(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxPlot for the length of the text in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEyCAYAAACPj9ldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7/8AH31Y4nSowGYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd33b21a650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>Awwh babs... you look so sad underneith that s...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>Tuesday�ll start with reflection �n then a lec...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>Whinging. My client&amp;amp;boss don't understand ...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "      <td>@TheLeagueSF Not Fun &amp;amp; Furious? The new ma...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>#3 woke up and was having an accident - &amp;quot;...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0</td>\n",
       "      <td>My bathtub drain is fired: it haz 1 job 2 do, ...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0</td>\n",
       "      <td>pears &amp;amp; Brie, bottle of Cabernet, and &amp;quo...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0</td>\n",
       "      <td>Have an invite for &amp;quot;Healthy Dining&amp;quot; ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0</td>\n",
       "      <td>Damnit I was really digging this season of Rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>0</td>\n",
       "      <td>Why do I keep looking...I know that what I rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "213           0  Awwh babs... you look so sad underneith that s...   \n",
       "226           0  Tuesday�ll start with reflection �n then a lec...   \n",
       "279           0  Whinging. My client&amp;boss don't understand ...   \n",
       "343           0  @TheLeagueSF Not Fun &amp; Furious? The new ma...   \n",
       "400           0  #3 woke up and was having an accident - &quot;...   \n",
       "464           0  My bathtub drain is fired: it haz 1 job 2 do, ...   \n",
       "492           0  pears &amp; Brie, bottle of Cabernet, and &quo...   \n",
       "747           0  Have an invite for &quot;Healthy Dining&quot; ...   \n",
       "957           0  Damnit I was really digging this season of Rea...   \n",
       "1064          0  Why do I keep looking...I know that what I rea...   \n",
       "\n",
       "      pre_clean_len  \n",
       "213             142  \n",
       "226             141  \n",
       "279             145  \n",
       "343             145  \n",
       "400             144  \n",
       "464             146  \n",
       "492             150  \n",
       "747             141  \n",
       "957             141  \n",
       "1064            141  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing beautifulsoup library for cleaning html special characters and converting to english charachters\n",
    "And testing the example of one specific tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly. \n",
      "______________________________________________________\n",
      "Parsed : Whinging. My client&boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&reviewed correctly. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "example1 = BeautifulSoup(df.text[279], 'lxml')\n",
    "print \"Original : \" + str(df.text[279])\n",
    "print \"______________________________________________________\"\n",
    "print \"Parsed : \" + str(example1.get_text() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleminating @mentions from the tweets as they carry no information for our tweets, and testing an example tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : @TheLeagueSF Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down \n",
      "______________________________________________________\n",
      "Parsed :  Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "example2 = re.sub(r'@[A-Za-z0-9]+','',df.text[343])\n",
    "print \"Original : \" + str(df.text[343])\n",
    "print \"______________________________________________________\"\n",
    "print \"Parsed : \" + str(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating links from the tweet if any and testing with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "______________________________________________________\n",
      "Parsed : @switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n"
     ]
    }
   ],
   "source": [
    "example3 = re.sub('https?://[A-Za-z0-9./]+','',df.text[0])\n",
    "print \"Original : \" + str(df.text[0])\n",
    "print \"______________________________________________________\"\n",
    "print \"Parsed : \" + str(example3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating UTF Bombs and testing with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuesday�ll start with reflection �n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers \n",
      "Tuesday�ll start with reflection �n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers \n",
      "Tuesday�ll start with reflection �n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers \n"
     ]
    }
   ],
   "source": [
    "testing = df.text[226].decode(\"utf-8-sig\")\n",
    "print testing\n",
    "testing.replace(u\"\\ufffd\", \"?\")\n",
    "print testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting hashtag into normal text for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : @machineplay I'm so sorry you're having to go through this. Again.  #therapyfail\n",
      "______________________________________________________\n",
      "Parsed :  machineplay I m so sorry you re having to go through this  Again    therapyfail\n"
     ]
    }
   ],
   "source": [
    "example5 = re.sub(\"[^a-zA-Z]\", \" \", df.text[175])\n",
    "print \"Original : \" + str(df.text[175])\n",
    "print \"______________________________________________________\"\n",
    "print \"Parsed : \" + str(example5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining our method for text cleaning of all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spring break in plain city it s snowing\n",
      "i just re pierced my ears\n",
      "i couldn t bear to watch it and i thought the ua loss was embarrassing\n",
      "it it counts idk why i did either you never talk to me anymore\n",
      "i would ve been the first but i didn t have a gun not really though zac snyder s just a doucheclown\n",
      "i wish i got to watch it with you i miss you and how was the premiere\n",
      "hollis death scene will hurt me severely to watch on film wry is directors cut not out now\n",
      "about to file taxes\n",
      "ahh ive always wanted to see rent love the soundtrack\n",
      "oh dear were you drinking out of the forgotten table drinks\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "#pattern to remove mentions and links from the tweet\n",
    "pattern1 = r'@[A-Za-z0-9]+'\n",
    "pattern2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pattern1, pattern2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    #replace the pattern with nothing '' in the tweet\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    #try - except to replace utf-BOMB\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    #last clean for all english letters only\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    #print \"Letters only : \" + str(lower_case)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "testing = df.text[10:20]\n",
    "\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "for t in test_result:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning whole data now, 1600k data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 10000 of 400000 has been processed\n",
      "Tweets 20000 of 400000 has been processed\n",
      "Tweets 30000 of 400000 has been processed\n",
      "Tweets 40000 of 400000 has been processed\n",
      "Tweets 50000 of 400000 has been processed\n",
      "Tweets 60000 of 400000 has been processed\n",
      "Tweets 70000 of 400000 has been processed\n",
      "Tweets 80000 of 400000 has been processed\n",
      "Tweets 90000 of 400000 has been processed\n",
      "Tweets 100000 of 400000 has been processed\n",
      "Tweets 110000 of 400000 has been processed\n",
      "Tweets 120000 of 400000 has been processed\n",
      "Tweets 130000 of 400000 has been processed\n",
      "Tweets 140000 of 400000 has been processed\n",
      "Tweets 150000 of 400000 has been processed\n",
      "Tweets 160000 of 400000 has been processed\n",
      "Tweets 170000 of 400000 has been processed\n",
      "Tweets 180000 of 400000 has been processed\n",
      "Tweets 190000 of 400000 has been processed\n",
      "Tweets 200000 of 400000 has been processed\n",
      "Tweets 210000 of 400000 has been processed\n",
      "Tweets 220000 of 400000 has been processed\n",
      "Tweets 230000 of 400000 has been processed\n",
      "Tweets 240000 of 400000 has been processed\n",
      "Tweets 250000 of 400000 has been processed\n",
      "Tweets 260000 of 400000 has been processed\n",
      "Tweets 270000 of 400000 has been processed\n",
      "Tweets 280000 of 400000 has been processed\n",
      "Tweets 290000 of 400000 has been processed\n",
      "Tweets 300000 of 400000 has been processed\n",
      "Tweets 310000 of 400000 has been processed\n",
      "Tweets 320000 of 400000 has been processed\n",
      "Tweets 330000 of 400000 has been processed\n",
      "Tweets 340000 of 400000 has been processed\n",
      "Tweets 350000 of 400000 has been processed\n",
      "Tweets 360000 of 400000 has been processed\n",
      "Tweets 370000 of 400000 has been processed\n",
      "Tweets 380000 of 400000 has been processed\n",
      "Tweets 390000 of 400000 has been processed\n",
      "Tweets 400000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "nums = [0,400000,800000,1200000,1600000]\n",
    "print \"Cleaning and parsing the tweets...\\n\"\n",
    "clean_tweet_texts = []\n",
    "for i in xrange(nums[0],nums[1]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print \"Tweets %d of %d has been processed\" % ( i+1, nums[1] )                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Saving Clean dataset to the csv ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that s a bummer you shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww that s a bummer you shoulda got david car...       0\n",
       "1  is upset that he can t update his facebook by ...       0\n",
       "2  i dived many times for the ball managed to sav...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it s not behaving at all i m mad why am i h...       0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "\n",
    "#saving to csv format\n",
    "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
